IT3030 - Deep Learning
----------------------

Solutions for the NTNU course IT3030.

## To do

- Handle the hidden layer shit correctly, and the no hidden layer shit.
- Regression without hidden layer should be fine. But should classification have multiple logits?
- Should regularization be scaled with number of training examples?
- What is the difference between alpha and lambda guys?
- In slides, regularization has a constant factor. Do we need it?
- Should bias also have a regularization term, since the assignment mentions that
- When to dump? After all epochs finished? What format?
- Skal vi serr ikke bruke NumPy greiene for å lagre til fil?
- Trenger vi faktor av 2 utenfor L2 loss?
- Trenger vi å regularisere loss-funksjonene eller bare håndtere det i deriverte?
- Sjekk at koden ikke kræsjer hvis mengden av data er multippel av 8.
